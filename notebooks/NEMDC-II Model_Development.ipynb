{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEM DATA CHALLENGE, 2nd Phase: \n",
    "## ML Model Development Module\n",
    "\n",
    "* Author: Sergio DÃ­az\n",
    "* Date: September 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    options = { 'test_score' :          0, # model error metric wrt test data \n",
    "                'historical_pwr':       0, # y_pred vs y_test on 5 months SCADA (time series)  \n",
    "                'validation_score' :    0, # model error metrics wrt Validation data\n",
    "                'validation_pwr'   :    0, # y_pred vs y_val on 24 hr scada dates (time-series)\n",
    "                'pwr_curve_suboptimal': 0, # pwr_curve with sub-optimal data pairs \n",
    "                'pwr_curve_predicted':  0, # ML predicted pwr_curve\n",
    "                'pwr_curve_residuals':  0, # Show outliers on Train_data\n",
    "                'save_model':           0,\n",
    "                'minutal_frequency':   10,\n",
    "              }\n",
    "    \n",
    "    # *******************READ HISTORICAL SCADA \n",
    "    df_hist_raw = read_historical_scada(options)\n",
    "    \n",
    "    # DATA Prepprocessing and model definition\n",
    "    df_hist, df_train, df_test, model = data_preprocessing (df_hist_raw, options)\n",
    "    \n",
    "    # Prediction using ML\n",
    "    predict(df_hist, df_train, df_test, model, options)\n",
    "    \n",
    "    # Score results comparing with validation 24scada\n",
    "    model_test(options)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict(df_hist, df_train, df_test, model, options):\n",
    "    ## ML MODEL CREATION\n",
    "\n",
    "    # define paths\n",
    "    cwd = os.getcwd()\n",
    "    # path to ML models\n",
    "    model_path = os.path.join(cwd,'models')\n",
    "\n",
    "    # ******************************************\n",
    "    # ML MODEL DEFINITION, TRAINING and SCORE\n",
    "\n",
    "    # Define features for model training and target label    \n",
    "    features = ['WNACWindSpeed', 'WNACAmbTemp']\n",
    "    target = 'WTURPower' \n",
    "    predict = 'energy_production'\n",
    "\n",
    "    # Loop over assets\n",
    "    assets = df_train.asset.unique()\n",
    "    for i, asset in enumerate(assets):\n",
    "        train_mask = df_train.asset==asset\n",
    "        test_mask = df_test.asset==asset\n",
    "\n",
    "        # Data definition for ML training\n",
    "        x_train = df_train.loc[train_mask, features]\n",
    "        y_train = df_train.loc[train_mask, target]\n",
    "        \n",
    "        x_test  = df_test.loc[test_mask, features]\n",
    "        y_test  = df_test.loc[test_mask, target]\n",
    "\n",
    "        model.fit(x_train, y_train)    \n",
    "\n",
    "        #********************\n",
    "        # SAVE ML MODEL ?\n",
    "        \n",
    "        if options['save_model'] == 1:\n",
    "            out_file = 'model_'+ asset + '.sav'\n",
    "            if not os.path.exists(model_path):\n",
    "                os.mkdir(result_path)\n",
    "            with open(model_path +  '/' + out_file, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "        #*******************\n",
    "        # MLModel metrics    \n",
    "        model_score = model.score(x_test,y_test)\n",
    "        y_pred = model.predict(x_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)*100\n",
    "        mae = mean_absolute_error(y_test,y_pred)*100\n",
    "        r2  = r2_score(y_test, y_pred)*100      \n",
    "\n",
    "        # Build dataframe with predicted values. Used for visualization\n",
    "        if i == 0:\n",
    "            df_prediction = pd.DataFrame(data=y_pred, index=x_test.index, columns=[predict])\n",
    "            df_prediction['asset']=asset                             \n",
    "        else:\n",
    "            df_concat = pd.DataFrame(data=y_pred, index=x_test.index, columns=[predict])\n",
    "            df_concat['asset']=asset                     \n",
    "            df_prediction = pd.concat([df_prediction,df_concat], axis=0)\n",
    "\n",
    "        # SHOWN TEST SCORES **************************************************\n",
    "        if options['test_score']==1:\n",
    "            if i==0: print('Machine Learing model score wiht historical scada') \n",
    "            print('Asset: {}'.format(asset) )\n",
    "            print(\"100x Mean squared error: {:.5f}\".format(mse))\n",
    "            print(\"100x r2_score: {:.5f}\".format(r2))\n",
    "            print(\"100x Mean absolute error: {:.5f}\".format(mae))\n",
    "            print(\"\")  \n",
    "\n",
    "        #*************************************************************\n",
    "        # PLOT Predictions for Y_pred on time series\n",
    "        if options['historical_pwr']==1:\n",
    "            mystyle()\n",
    "            dfA = df_hist[df_hist.asset == asset]\n",
    "            dfB = df_prediction[df_prediction.asset == asset]\n",
    "\n",
    "            fig, ax1 = plt.subplots( figsize=(9,4) )\n",
    "            dfA[target].plot(ax=ax1, lw=.5)\n",
    "            dfB[predict].plot(style='g.-', lw=0, ax=ax1)\n",
    "            ax1.set_title('Historical SCADA records for asset: '+ asset)\n",
    "            ax1.set_ylabel(target)\n",
    "            ax1.set_xlabel('month-day')\n",
    "            myFmt = mdates.DateFormatter('%m-%d')\n",
    "            ax1.xaxis.set_major_formatter(myFmt)\n",
    "            plt.setp( ax1.xaxis.get_majorticklabels(), rotation=0 );\n",
    "            if i ==0:\n",
    "                pass\n",
    "                #sfig('raw_pwr')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SCORE USING EVALUATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_test(options):\n",
    "\n",
    "    # define paths\n",
    "    cwd = os.getcwd()\n",
    "    # path to ML models\n",
    "    model_path = os.path.join(cwd,'models')\n",
    "    \n",
    "    # *****************************************\n",
    "    # READ EVALUATION SCADA DATA PROVIDED BY NEM\n",
    "\n",
    "    df_scada = read_evaluation_scada()     \n",
    "\n",
    "    # Define features for model fit and target label    \n",
    "    features = ['WNACWindSpeed', 'WNACAmbTemp']\n",
    "    target = 'WTURPower'\n",
    "    predict = 'energy_production'\n",
    "\n",
    "    # Loop over assets\n",
    "    assets = df_scada.asset.unique()\n",
    "    for i, asset in enumerate(assets):\n",
    "\n",
    "        # DATA Selection\n",
    "        mask = df_scada.asset==asset\n",
    "        x_eval = df_scada.loc[mask, features]\n",
    "        y_eval = df_scada.loc[mask, target]\n",
    "\n",
    "        # read previously trained ML odel\n",
    "        in_file = 'model_'+ asset + '.sav'\n",
    "        with open(model_path +  '/' + in_file, 'rb') as f:\n",
    "            model = pickle.load(f) \n",
    "\n",
    "        # ML Prediction\n",
    "        y_pred = model.predict(x_eval)\n",
    "        mse = mean_squared_error(y_eval, y_pred)*100\n",
    "        mae = mean_absolute_error(y_eval,y_pred)*100\n",
    "        r2  = r2_score(y_eval, y_pred)*100\n",
    "\n",
    "        # Build dataframe with predicted values. Used for visualization\n",
    "        if i == 0:\n",
    "            df_prediction = pd.DataFrame(data=y_pred, index=x_eval.index, columns=[predict])\n",
    "            df_prediction['asset']=asset                             \n",
    "        else:\n",
    "            df_concat = pd.DataFrame(data=y_pred, index=x_eval.index, columns=[predict])\n",
    "            df_concat['asset']=asset                     \n",
    "            df_prediction = pd.concat([df_prediction,df_concat], axis=0)\n",
    "\n",
    "        # SHOW Model VALIDATION score\n",
    "        if options['validation_score']==1:\n",
    "            if i==0: print('Machine Learing model score for evaluation scada')\n",
    "            print('Asset: {}'.format(asset) )    \n",
    "            print(\"100x Mean squared error: {:.5f}\".format(mse))\n",
    "            print(\"100x r2_score: {:.5f}\".format(r2))\n",
    "            print(\"100x Mean absolute error: {:.5f}\".format(mae))\n",
    "            print('')\n",
    "\n",
    "        # PLOT Predictions TIME SERIES\n",
    "        if options['validation_pwr']==1:\n",
    "            mystyle()\n",
    "            dfA = df_scada[df_scada.asset == asset]\n",
    "            dfB = df_prediction[df_prediction.asset == asset]\n",
    "\n",
    "            fig, ax1 = plt.subplots( figsize=(9.5,6) )\n",
    "            dfA[target].plot(ax=ax1, marker='.', lw=.3)\n",
    "            dfB[predict].plot(style='g.-', lw=0, ax=ax1)\n",
    "            ax1.set_title('Prediction Output for asset: '+ asset)\n",
    "            ax1.set_ylabel(target)\n",
    "            plt.setp( ax1.xaxis.get_majorticklabels(), rotation=0 );\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(df_hist, options):\n",
    "    ''' Data cleaning using preliminar ML model '''\n",
    "    \n",
    "    df_hist = df_hist.dropna(axis=0)  # remove any row with nans\n",
    "    \n",
    "    # ******************************************\n",
    "    # CLEAN HISTORICAL scada USING ML and residuals\n",
    "\n",
    "    # Define features for model training and target label    \n",
    "    features = ['WNACWindSpeed', 'WNACAmbTemp']\n",
    "    target = 'WTURPower' \n",
    "\n",
    "    # Loop over assets\n",
    "    assets = df_hist.asset.unique()\n",
    "    for i, asset in enumerate(assets):\n",
    "        asset_mask = df_hist.asset==asset\n",
    "\n",
    "        # Data definition for ML training\n",
    "        y_data = df_hist.loc[asset_mask, target]\n",
    "        x_data = df_hist.loc[asset_mask, features]\n",
    "        x_train, x_test, y_train, y_test, model = ML_model(x_data,y_data) #FUN Call\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(x_train)\n",
    "        # calculate squared error\n",
    "        error= 1000*(y_pred - y_train)**2\n",
    "        error_lim = 1.0 # error limit for valid values\n",
    "        \n",
    "        if i == 0:\n",
    "            df_train = x_train.copy()\n",
    "            df_train[target] = y_train\n",
    "            df_train['asset'] = asset\n",
    "            df_train['pred_pwr'] = y_pred\n",
    "            df_train['error'] = error\n",
    "            df_train['valid'] = df_train.error <= error_lim\n",
    "        else:\n",
    "            df_concat = x_train.copy()\n",
    "            df_concat[target] = y_train\n",
    "            df_concat['asset'] = asset\n",
    "            df_concat['pred_pwr'] = y_pred\n",
    "            df_concat['error'] = error\n",
    "            df_concat['valid'] = df_concat.error <= error_lim            \n",
    "            df_train = pd.concat([df_train, df_concat], axis=0)\n",
    "\n",
    "        if i == 0:\n",
    "            df_test = x_test.copy()\n",
    "            df_test[target] = y_test\n",
    "            df_test['asset'] = asset                             \n",
    "        else:\n",
    "            df_concat2 = x_test.copy()\n",
    "            df_concat2[target] = y_test\n",
    "            df_concat2['asset'] = asset                     \n",
    "            df_test = pd.concat([df_test, df_concat2], axis=0)                  \n",
    " \n",
    "        # plot Power Curves and residuals\n",
    "\n",
    "        if options['pwr_curve_residuals']==1:   \n",
    "            \n",
    "            dfA = df_hist[df_hist.asset==asset]\n",
    "            dfB = df_train[df_train.asset==asset]\n",
    "            dfB_remove = dfB[dfB.valid==False]\n",
    "            dfC = df_test[df_test.asset==asset]\n",
    "\n",
    "            #fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(9,18) )\n",
    "            fig, (ax1,ax2) = plt.subplots(2,1, figsize=(9,12) )\n",
    "            s=10 # marker size\n",
    "            \n",
    "            dfA.plot.scatter('WNACWindSpeed', 'WTURPower', ax=ax1, marker='.', s=s, c='b', label='5-Months-Historical')\n",
    "            dfB.plot.scatter('WNACWindSpeed', 'WTURPower', ax=ax2, marker='.', s=s, c='g', label='MLM_Train_Data')\n",
    "            dfB_remove.plot.scatter('WNACWindSpeed', 'WTURPower', ax=ax2, marker='.', s=s, c='r', label='Outliers')  \n",
    "            \n",
    "            ax1.set_title('Power Curve for asset: '+ asset)\n",
    "            ax2.set_title('Power Curve for asset: '+ asset)\n",
    "            #dfC.plot.scatter('WNACWindSpeed', 'WTURPower', \n",
    "            #                 ax=ax3, marker='.', s=s, c='b', label ='MLM_Tests_Data')\n",
    "            \n",
    "            \n",
    "        # SHOW Power Curves and sub_optimal datapoints\n",
    "        \n",
    "        if options['pwr_curve_suboptimal']==1:   \n",
    "            df_A = df_hist[df_hist.asset==asset]\n",
    "            \n",
    "            # Remove near-zero Power values at windspeed higher than 0.1\n",
    "            mask = (df_A.WTURPower < 0.001) & (df_A.WNACWindSpeed > 0.10) \n",
    "            df_B = df_A[mask]\n",
    "            # Remove sub-optimal Power values \n",
    "            mask = (df_A.WTURPower < 0.95) & (df_A.WNACWindSpeed > 0.35) \n",
    "            df_C = df_A[mask]\n",
    "\n",
    "            fig, (ax1) = plt.subplots(1,1, figsize=(7,5) )\n",
    "            s=2 # marker size\n",
    "            df_A.plot.scatter('WNACWindSpeed','WTURPower', ax=ax1, marker='.',s=1,c='g',label='OK')\n",
    "            df_C.plot.scatter('WNACWindSpeed','WTURPower', ax=ax1, marker='+',s=15,c='m',label ='NOK under-rated-pwr')    \n",
    "            df_B.plot.scatter('WNACWindSpeed','WTURPower', ax=ax1, marker='x',s=15,c='b',label='NOK zero-pwr')\n",
    "            ax1.set_title('Power Curve for asset: '+ asset)\n",
    "            \n",
    "        # plot PREDICTED Power Curves \n",
    "        \n",
    "        if options['pwr_curve_predicted']==1:   \n",
    "            df_A = df_train[df_train.asset==asset]\n",
    "            fig, (ax1) = plt.subplots(1,1, figsize=(9,6) )\n",
    "            df_A.plot.scatter('WNACWindSpeed', 'pred_pwr', ax=ax1, marker='.', s=10, c='k')\n",
    "            ax1.set_title('Power Curve for asset: '+ asset)\n",
    "            ax1.set_ylabel('Predicted WTURPower')\n",
    "            \n",
    "    ## RESIDUALS INSPECTION (visual aid during development)\n",
    "    execute = 0\n",
    "    if execute == 1:\n",
    "        df_train.head()\n",
    "        df_train.describe()\n",
    "        df_train.quantile(.95)\n",
    "        print ('Descriptive stats for error dataframe')\n",
    "        print ( df_train.pivot( columns='asset', values='error').describe() )\n",
    "        print ( ' 95% quantile for error column')\n",
    "        print ( df_train.pivot( columns='asset', values='error').quantile(.95) )\n",
    "        print ('')\n",
    "        # visualize residuals kde\n",
    "        ast='A001'\n",
    "        fig, ax1 = plt.subplots(figsize=(9,3) )\n",
    "        sns.kdeplot(df_train.loc[df_train.asset==ast,'error'], label=ast, ax=ax1)\n",
    "    \n",
    "    # DATA CLEANING \n",
    "    # set execute to 1 to remove outliers from ml model training data\n",
    "    execute = 0\n",
    "    if execute==1: \n",
    "        # Remove residuals higher than 1.0\n",
    "        #df_train =  df_train[df_train.valid==True]\n",
    "        \n",
    "        # Remove near-zero Power values at windspeed higher than 0.1\n",
    "        mask = np.invert((df_train.WTURPower < 0.001) & (df_train.WNACWindSpeed > 0.10)) \n",
    "        df_train = df_train[mask]\n",
    "\n",
    "        # Remove sub-optimal Power values \n",
    "        mask = np.invert((df_train.WTURPower < 0.95) & (df_train.WNACWindSpeed > 0.35)) \n",
    "        df_train = df_train[mask]\n",
    "    \n",
    "    return df_hist, df_train, df_test, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ML_model(x_data, y_data):\n",
    "    ''' Machine Learning Model Definition '''\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    #from xgboost.sklearn import XGBRegressor\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=.3, random_state=0) \n",
    "    \n",
    "    selector = 3\n",
    "    \n",
    "    if selector == 1:\n",
    "        model = DecisionTreeRegressor(max_depth=7, random_state=0, splitter='best')\n",
    "    elif selector == 2:\n",
    "        model = GradientBoostingRegressor(n_estimators=100,  learning_rate=0.1, max_depth=7, random_state=0, \n",
    "                                          loss='ls')\n",
    "    elif selector==3:    \n",
    "        model = RandomForestRegressor(max_depth=9,random_state=0)\n",
    "\n",
    "    elif selector==4:\n",
    "        params = {\n",
    "        'objective': \"reg:linear\",\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 1.0,\n",
    "        'silent': 1.0,\n",
    "        'n_estimators': 10}\n",
    "        model = XGBRegressor(**params)\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test, model      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_historical_scada(options):\n",
    "    '''select and read 1, 2, or 10 minutal SCADA datasets to train ML model'''\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    frequency = options['minutal_frequency']\n",
    "    \n",
    "    if frequency == 10:\n",
    "        in_path = os.path.join(cwd,'datasets', 'historical', 'raw')\n",
    "        file_ext =  '-10m.csv'\n",
    "        \n",
    "    elif frequency == 2:\n",
    "        in_path = os.path.join(cwd,'datasets', 'historical', 'raw', '2m_results')\n",
    "        file_ext =  '-2m.csv'\n",
    "        \n",
    "    elif frequency == 1:\n",
    "        in_path = os.path.join(cwd,'datasets', 'historical', 'raw', '1m_results')\n",
    "        file_ext =  '-1m.csv'       \n",
    "\n",
    "    \n",
    "    months = ['04','05','06','07','08']\n",
    "    for i, month in enumerate (months):\n",
    "        in_file = '2015-'+ month + file_ext        \n",
    "        in_csv = os.path.join(in_path,in_file)\n",
    "        if i==0:           \n",
    "            df = pd.read_csv(in_csv)\n",
    "        else:\n",
    "            df_cat = pd.read_csv(in_csv)\n",
    "            df = pd.concat([df,df_cat], axis=0)\n",
    "            \n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    return df  \n",
    "\n",
    "\n",
    "def read_evaluation_scada():\n",
    "    ''' '''\n",
    "    cwd = os.getcwd()\n",
    "    in_path = os.path.join(cwd,'datasets', 'evaluation', 'online')\n",
    "    dates = ['2015-09-01',\n",
    "             '2015-09-09',\n",
    "             '2015-09-17',\n",
    "             '2015-09-25',\n",
    "             '2015-10-03',\n",
    "             '2015-10-10']\n",
    "    for i, date in enumerate (dates):\n",
    "        in_file = date +'.csv'         \n",
    "        in_csv  = os.path.join(in_path,in_file)\n",
    "        if i==0:           \n",
    "            df = pd.read_csv(in_csv)\n",
    "        else:\n",
    "            df_concat = pd.read_csv(in_csv)\n",
    "            df = pd.concat([df,df_concat], axis=0)\n",
    "            \n",
    "    df['datetime'] = pd.to_datetime(df['unixtime'], unit='s')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    ## DATA CLEANING\n",
    "    df = df.dropna(axis=0) \n",
    "    \n",
    "    # clean evaluation scada data?\n",
    "    execute=0\n",
    "    if execute==1:\n",
    "        # Remove near-zero Power values at windspeed higher than 0.1\n",
    "        mask = np.invert((df.WTURPower<0.001) & (df.WNACWindSpeed>0.1)) \n",
    "        df = df[mask]\n",
    "        \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mystyle():\n",
    "    '''set some default parameters for visualizations'''\n",
    "    \n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.rcParams ['axes.grid']=False \n",
    "    plt.rcParams ['axes.spines.left']   = True   # display axis spines\n",
    "    plt.rcParams ['axes.spines.bottom'] = True\n",
    "    plt.rcParams ['axes.spines.top']    = False\n",
    "    plt.rcParams ['axes.spines.right']  = False \n",
    "    \n",
    "def sfig(figname='myfig'):\n",
    "    \n",
    "    name = figname+'.png'\n",
    "    path = r'C:\\Home00Ser\\Python\\NEM Challenge\\Fase2\\report\\figs'\n",
    "    fname = os.path.join(path,name)\n",
    "\n",
    "    plt.savefig(fname, dpi=900, facecolor='w', edgecolor='w',\n",
    "            orientation='portrait', papertype=None, format=None,\n",
    "            transparent=False, bbox_inches='tight', pad_inches=0.1,\n",
    "            frameon=None)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' VERSION CTRL\n",
    "\n",
    "V01: ok: cleaned and commented for file sharing.\n",
    "     ok: pending better historical data cleaning\n",
    "     ok: pending model scoring using evaluation scada data\n",
    "\n",
    "V02: ok: model scoring on evaluation scada\n",
    "     ok: check data cleaning and prediction of near zero power values\n",
    "     \n",
    "V03: ok: Script better orginized in functions\n",
    "\n",
    "V04: ok: Improve data cleaning using ML model and residuals\n",
    "     ok: assess impact of cleaing method\n",
    "     ok: remove previous data preprocessing strategy: done\n",
    "     ok: write MAIN function\n",
    "\n",
    "V05: ok: train models with 2-minutal datasets and quantify gain. Score imporves, go for 1min\n",
    "     \n",
    "V06: pend: detect Nan values in input files and avoid prediction. improve results for 09.09\n",
    "     ok:   perform data cleaning on train_data only.  \n",
    "     pend: try XGB     \n",
    "     \n",
    "     \n",
    "''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
